{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "748e201e-7482-4933-abbc-5abdb6d832e8",
   "metadata": {},
   "source": [
    "# Introduction to LLamaIndex\n",
    "LlamaIndex is a complete toolkit for creating LLM-powered agents over your data using indexes and workflows. For this course we’ll focus on three main parts that help build agents in LlamaIndex: Components, Agents and Tools and Workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7fefb9-97c0-4e94-832d-291c2b60a0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-llms-huggingface-api llama-index-embeddings-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df613240-7920-4302-93da-1e6b8fb7af75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve HF_TOKEN from the environment variables\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=100,\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "response = llm.complete(\"Hello, how are you?\")\n",
    "print(response)\n",
    "# I am good, how can I help you today?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2a363-e427-4358-bcfb-47b90545f0de",
   "metadata": {},
   "source": [
    "## What are components in LLamaIndex?\n",
    "While LlamaIndex has many components, we’ll focus specifically on the QueryEngine component. Why? Because it can be used as a Retrieval-Augmented Generation (RAG) tool for an agent.\n",
    "\n",
    "So, what is RAG? LLMs are trained on enormous bodies of data to learn general knowledge. However, they may not be trained on relevant and up-to-date data. RAG solves this problem by finding and retrieving relevant information from your data and giving that to the LLM.\n",
    "\n",
    "Now, think about how Alfred works:\n",
    "\n",
    "1. You ask Alfred to help plan a dinner party\n",
    "2. Alfred needs to check your calendar, dietary preferences, and past successful menus\n",
    "3. The `QueryEngine` helps Alfred find this information and use it to plan the dinner party\n",
    "\n",
    "This makes the `QueryEngine` a **key component for building agentic RAG workflows** in LlamaIndex. Just as Alfred needs to search through your household information to be helpful, any agent needs a way to find and understand relevant data. The QueryEngine provides exactly this capability.\n",
    "\n",
    "Now, let’s dive a bit deeper into the components and see how you can combine components to create a RAG pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce32bb4e-d2bd-470c-a726-6dcc7d2c65d9",
   "metadata": {},
   "source": [
    "There are five key stages within RAG, which in turn will be a part of most larger applications you build. These are:\n",
    "\n",
    "1. **Loading**: this refers to getting your data from where it lives — whether it’s text files, PDFs, another website, a database, or an API — into your workflow. LlamaHub provides hundreds of integrations to choose from.\n",
    "2. **Indexing**: this means creating a data structure that allows for querying the data. For LLMs, this nearly always means creating vector embeddings. Which are numerical representations of the meaning of the data. Indexing can also refer to numerous other metadata strategies to make it easy to accurately find contextually relevant data based on properties.\n",
    "3. **Storing**: once your data is indexed you will want to store your index, as well as other metadata, to avoid having to re-index it.\n",
    "4. **Querying**: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.\n",
    "5. **Evaluation**: a critical step in any flow is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a029f0f-dd6c-42ad-8996-fc56f3a6dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1811a0b4-e38e-49b3-9fa1-139d59a0ff14",
   "metadata": {},
   "source": [
    "We will be using personas from the `dvilasuero/finepersonas-v0.1-tiny dataset`. This dataset contains 5K personas that will be attending the party!\n",
    "\n",
    "Let's load the dataset and store it as files in the `data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f8d77ee-a75e-4c73-93bd-ecdd68f45523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3136bdb3c76b436bb3d5fd44a7c47e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/618 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f671ab50cac44d196d909774b71b892",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/35.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c1b3c1f5144cb8b0fbeddd0fbdb9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = load_dataset(path=\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "for i, persona in enumerate(dataset):\n",
    "    with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\") as f:\n",
    "        f.write(persona[\"persona\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f871f03-5abf-4747-afbc-a2e5fa807c89",
   "metadata": {},
   "source": [
    "## Loading and ebbedding documents\n",
    "\n",
    "Prima di accedere ai dati dobbiamo caricarli, abbiamo 3 modi per farlo:\n",
    "1. `SimpleDirectoryReader`: A built-in loader for various file types from a local directory.\n",
    "2. `LlamaParse`: LlamaParse, LlamaIndex’s official tool for PDF parsing, available as a managed API.\n",
    "3. `LlamaHub`: A registry of hundreds of data-loading libraries to ingest data from any source.\n",
    "\n",
    "\n",
    "The simplest way to load data is with `SimpleDirectoryReader`. This versatile component can load various file types from a folder and convert them into Document objects that LlamaIndex can work with. Let’s see how we can use SimpleDirectoryReader to load data from a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "767435c4-fcae-4c18-8177-c04e5556da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
    "documents = reader.load_data()\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef83aec-d1bd-41ca-bc87-f7526ce9f7cf",
   "metadata": {},
   "source": [
    "After loading our documents, we need to break them into smaller pieces called Node objects. A Node is just a chunk of text from the original document that’s easier for the AI to work with, while it still has references to the original Document object.\n",
    "\n",
    "The `IngestionPipeline` helps us create these nodes through two key transformations.\n",
    "\n",
    "1. `SentenceSplitter` breaks down documents into manageable chunks by splitting them at natural sentence boundaries.\n",
    "2. `HuggingFaceEmbedding` converts each chunk into numerical embeddings - vector representations that capture the semantic meaning in a way AI can process efficiently.\n",
    "   \n",
    "This process helps us organise our documents in a way that’s more useful for searching and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495df58-2b7a-4104-954d-6c066e8143d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
