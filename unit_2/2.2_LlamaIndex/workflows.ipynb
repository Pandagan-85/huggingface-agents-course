{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows in LlamaIndex\n",
    "\n",
    "\n",
    "This notebook is part of the [Hugging Face Agents Course](https://www.hf.co/learn/agents-course), a free Course from beginner to expert, where you learn to build Agents.\n",
    "\n",
    "![Agents course share](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/communication/share.png)\n",
    "\n",
    "## Let's install the dependencies\n",
    "\n",
    "We will install the dependencies for this unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-vector-stores-chroma llama-index-utils-workflow llama-index-llms-huggingface-api pyvis -U -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, let's log in to Hugging Face to use serverless Inference APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Agentic Workflows in LllamaIndex\n",
    "\n",
    "A workflow in LlamaIndex provides a structured way to organize your code into sequential and manageable steps.\n",
    "\n",
    "Such a workflow is created by defining `Steps` which are triggered by `Events`, and themselves emit Events to trigger further steps. Let’s take a look at Alfred showing a LlamaIndex workflow for a RAG task.\n",
    "\n",
    "Workflows offer several key benefits:\n",
    "\n",
    "- Clear organization of code into discrete steps\n",
    "- Type-safe communication between steps\n",
    "- Built-in state management\n",
    "- Support for both simple and complex agent interactions\n",
    "\n",
    "As you might have guessed, **workflows strike a great balance between the autonomy of agents while maintaining control over the overall workflow**.\n",
    "\n",
    "So, let’s learn how to create a workflow ourselves!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Workflow Creation\n",
    "\n",
    "We can start by creating a simple workflow. We use the `StartEvent` and `StopEvent` classes to define the start and stop of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index-utils-workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
    "\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result=\"Hello, world!\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the workflow by calling `w.run()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Multiple Steps\n",
    "\n",
    "To connect multiple steps, we **create custom events that carry data between steps**. To do so, we need to add an Event that is passed between the steps and transfers the output of the first step to the second step.\n",
    "\n",
    "\n",
    "\n",
    "We can also create multi-step workflows. Here we pass the event information between steps. Note that we can use type hinting to specify the event type and the flow of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Finished processing: Step 1 complete'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type hinting is important here, as it ensures that the workflow is executed correctly. Let’s complicate things a bit more!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops and Branches\n",
    "\n",
    "The type hinting is the most powerful part of workflows because it allows us to create branches, loops, and joins to facilitate more complex workflows.\n",
    "\n",
    "Let’s show an example of **creating a loop** by using the union operator `|`. In the example below, we see that the LoopEvent is taken as input for the step and can also be returned as output.\n",
    "\n",
    "We can also use type hinting to create branches and loops. Note that we can use the `|` operator to specify that the step can return multiple types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad thing happened\n",
      "Good thing happened\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: First step complete.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "import random\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Bad thing happened\")\n",
    "            return LoopEvent(loop_output=\"Back to step one.\")\n",
    "        else:\n",
    "            print(\"Good thing happened\")\n",
    "            return ProcessingEvent(intermediate_result=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drawing Workflows\n",
    "\n",
    "We can also draw workflows using the `draw_all_possible_flows` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflow_all_flows.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![drawing](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/workflow-draw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last cool trick that we will cover in the course, which is the ability to add state to the workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Management\n",
    "\n",
    "Instead of passing the event information between steps, we can use the `Context` type hint to pass information between steps. \n",
    "This might be useful for long running workflows, where you want to store information between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of France?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: Step 1 complete'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event, Context\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent, ctx: Context) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        await ctx.set(\"query\", \"What is the capital of France?\")\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent, ctx: Context) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        query = await ctx.get(\"query\")\n",
    "        print(f\"Query: {query}\")\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last cool trick that we will cover in the course, which is the ability to add state to the workflow.\n",
    "\n",
    "Instead of manual workflow creation, we can use the `AgentWorkflow` class **to create a multi-agent workflow**. The AgentWorkflow uses Workflow Agents to allow you to create a system of one or more agents that can collaborate and hand off tasks to each other based on their specialized capabilities. This enables building complex agent systems where different agents handle different aspects of a task. Instead of importing classes from `llama_index.core.agent`, we will import the agent classes from `llama_index.core.agent.workflow`. One agent must be designated as the root agent in the `AgentWorkflow` constructor. When a user message comes in, it is first routed to the root agent.\n",
    "\n",
    "Each agent can then:\n",
    "\n",
    "- Handle the request directly using their tools\n",
    "- Handoff to another agent better suited for the task\n",
    "- Return a response to the user\n",
    "\n",
    "Let’s see how to create a multi-agent workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent Workflows\n",
    "\n",
    "We can also create multi-agent workflows. Here we define two agents, one that multiplies two integers and one that adds two integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={}, blocks=[TextBlock(block_type='text', text='5 and 3 add up to 8.')]), tool_calls=[ToolCallResult(tool_name='handoff', tool_kwargs={'to_agent': 'add_agent', 'reason': 'The user wants to add two numbers, and the add_agent is better suited for this task.'}, tool_id='b2468032-c1ec-474e-8845-61cde9a3c078', tool_output=ToolOutput(content='Agent add_agent is now handling the request due to the following reason: The user wants to add two numbers, and the add_agent is better suited for this task..\\nPlease continue with the current request.', tool_name='handoff', raw_input={'args': (), 'kwargs': {'to_agent': 'add_agent', 'reason': 'The user wants to add two numbers, and the add_agent is better suited for this task.'}}, raw_output='Agent add_agent is now handling the request due to the following reason: The user wants to add two numbers, and the add_agent is better suited for this task..\\nPlease continue with the current request.', is_error=False), return_direct=True), ToolCallResult(tool_name='add', tool_kwargs={'a': 5, 'b': 3}, tool_id='5276c50c-1f66-4781-8211-7dff0b565365', tool_output=ToolOutput(content='8', tool_name='add', raw_input={'args': (), 'kwargs': {'a': 5, 'b': 3}}, raw_output=8, is_error=False), return_direct=False)], raw=ChatCompletionStreamOutput(choices=[ChatCompletionStreamOutputChoice(delta=ChatCompletionStreamOutputDelta(role='assistant', content='.', tool_call_id=None, tool_calls=None), index=0, finish_reason=None, logprobs=None)], created=1749546237, id='', model='Qwen/Qwen2.5-Coder-32B-Instruct', system_fingerprint='3.2.1-sha-4d28897', usage=None, object='chat.completion.chunk'), current_agent_name='add_agent')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
    "from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "from llama_index.core.agent.workflow import AgentWorkflow\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
    "multiply_agent = ReActAgent(\n",
    "    name=\"multiply_agent\",\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools=[multiply], \n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name=\"add_agent\",\n",
    "    description=\"Is able to add two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools=[add], \n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent tools can also modify the workflow state we mentioned earlier. Before starting the workflow, we can provide an initial state dict that will be available to all agents. The state is stored in the state key of the workflow context. It will be injected into the state_prompt which augments each new user message.\n",
    "\n",
    "Let’s inject a counter to count function calls by modifying the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# Define some tools\n",
    "async def add(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    # update our count\n",
    "    cur_state = await ctx.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"] += 1\n",
    "    await ctx.set(\"state\", cur_state)\n",
    "\n",
    "    return a + b\n",
    "\n",
    "async def multiply(ctx: Context, a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    # update our count\n",
    "    cur_state = await ctx.get(\"state\")\n",
    "    cur_state[\"num_fn_calls\"] += 1\n",
    "    await ctx.set(\"state\", cur_state)\n",
    "\n",
    "    return a * b\n",
    "\n",
    "...\n",
    "\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\",\n",
    "    initial_state={\"num_fn_calls\": 0},\n",
    "    state_prompt=\"Current state: {state}. User message: {msg}\",\n",
    ")\n",
    "\n",
    "# run the workflow with context\n",
    "ctx = Context(workflow)\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\", ctx=ctx)\n",
    "\n",
    "# pull out and inspect the state\n",
    "state = await ctx.get(\"state\")\n",
    "print(state[\"num_fn_calls\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
